#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/gemma2/modular_gemma2.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_gemma2.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2024 Google Inc. HuggingFace Inc. team. All rights reserved.
#
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from typing import Callable, List, Optional, Tuple, Union

import torch
import torch.nn as nn
from transformers.modeling_attn_mask_utils import (
    _prepare_4d_causal_attention_mask,
    _prepare_4d_causal_attention_mask_for_sdpa)
from transformers.activations import ACT2FN
from transformers.cache_utils import Cache, HybridCache, StaticCache
from transformers.generation import GenerationMixin
from transformers.modeling_flash_attention_utils import FlashAttentionKwargs
from transformers.modeling_outputs import (
    BaseModelOutputWithPast,
    CausalLMOutputWithPast,
)
from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS
from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from transformers.processing_utils import Unpack
from transformers.utils import (
    add_start_docstrings,
    add_start_docstrings_to_model_forward,
    is_torchdynamo_compiling,
    logging,
    replace_return_docstrings,
)
from transformers.utils.deprecation import deprecate_kwarg

# from transformers.models.gemma2.configuration_gemma2 import Gemma2Config


from dataclasses import dataclass
from model_config import Gemma2Config

logger = logging.get_logger(__name__)

_CHECKPOINT_FOR_DOC = "google/gemma2-7b"
_CONFIG_FOR_DOC = "Gemma2Config"


class Gemma2RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.zeros(dim))

    def _norm(self, x):
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        output = self._norm(x.float())
        # Llama does x.to(float16) * w whilst Gemma2 is (x * w).to(float16)
        # See https://github.com/huggingface/transformers/pull/29402
        output = output * (1.0 + self.weight.float())
        return output.type_as(x)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.eps}"


class Gemma2MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_activation]

    def forward(self, x):
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        return down_proj


class Gemma2ActionMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_activation]

    def forward(self, x):
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        return down_proj


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(
        batch, num_key_value_heads, n_rep, slen, head_dim
    )
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    dropout: float = 0.0,
    scaling: Optional[float] = None,
    softcap: Optional[float] = None,
    **kwargs,
) -> Tuple[torch.Tensor, torch.Tensor]:
    if scaling is None:
        scaling = module.head_dim**-0.5

    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling

    if softcap is not None:
        attn_weights = attn_weights / softcap
        attn_weights = torch.tanh(attn_weights)
        attn_weights = attn_weights * softcap
    if attention_mask is not None:  # no matter the length, we just slice it
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    # upcast attention to fp32
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(
        query.dtype
    )
    attn_weights = nn.functional.dropout(
        attn_weights, p=dropout, training=module.training
    )
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()
    return attn_output, attn_weights


class Gemma2Attention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: Gemma2Config, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.head_dim = getattr(
            config, "head_dim", config.hidden_size // config.num_attention_heads
        )
        self.num_key_value_groups = (
            config.num_attention_heads // config.num_key_value_heads
        )
        self.scaling = config.query_pre_attn_scalar**-0.5
        self.attention_dropout = self.config.attention_dropout
        self.action_expert_config = config.action_expert_config

        self.is_causal = True

        self.q_proj = nn.Linear(
            config.hidden_size,
            config.num_attention_heads * self.head_dim,
            bias=config.attention_bias,
        )
        self.k_proj = nn.Linear(
            config.hidden_size,
            config.num_key_value_heads * self.head_dim,
            bias=config.attention_bias,
        )
        self.v_proj = nn.Linear(
            config.hidden_size,
            config.num_key_value_heads * self.head_dim,
            bias=config.attention_bias,
        )
        self.o_proj = nn.Linear(
            config.num_attention_heads * self.head_dim,
            config.hidden_size,
            bias=config.attention_bias,
        )

        self.action_q_proj = nn.Linear(
            self.action_expert_config.hidden_size,
            config.num_attention_heads * self.head_dim,
            bias=config.attention_bias,
        )
        self.action_k_proj = nn.Linear(
            self.action_expert_config.hidden_size,
            config.num_key_value_heads * self.head_dim,
            bias=config.attention_bias,
        )
        self.action_v_proj = nn.Linear(
            self.action_expert_config.hidden_size,
            config.num_key_value_heads * self.head_dim,
            bias=config.attention_bias,
        )
        self.action_o_proj = nn.Linear(
            config.num_attention_heads * self.head_dim,
            self.action_expert_config.hidden_size,
            bias=config.attention_bias,
        )

        self.qkvs_proj = [
            (self.q_proj, self.k_proj, self.v_proj),
            (self.action_q_proj, self.action_k_proj, self.action_v_proj),
        ]
        self.os_proj = [self.o_proj, self.action_o_proj]
        self.attn_logit_softcapping = self.config.attn_logit_softcapping
        self.sliding_window = config.sliding_window if not bool(layer_idx % 2) else None

    def forward(
        self,
        multi_hidden_states: Tuple[Optional[torch.Tensor], Optional[torch.Tensor]], # æ˜Žç¡®ç±»åž‹
        position_embeddings: Tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor],
        past_key_value: Optional[Cache] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Tuple[Tuple[Optional[torch.Tensor], Optional[torch.Tensor]], Optional[torch.Tensor]]: # è°ƒæ•´è¿”å›žç±»åž‹
        
        first_valid_hidden_state = next((hs for hs in multi_hidden_states if hs is not None), None)
        if first_valid_hidden_state is None:
             # Handle case where both inputs are None (should ideally not happen if called correctly)
             return (None, None), None

        # Unpack the 3D shape correctly
        batch_size, query_seq_len, _ = first_valid_hidden_state.shape

        qkvs = []
        input_lens = []
        hidden_shapes = [] # å­˜å‚¨æ¯ä¸ªæµçš„ hidden_shape

        # --- QKV æŠ•å½± ---
        for i, hidden_states in enumerate(multi_hidden_states):
            if hidden_states is None:
                qkvs.append((None, None, None))
                input_lens.append(0)
                hidden_shapes.append(None)
                continue

            qkv_proj = self.qkvs_proj[i] # ç›´æŽ¥ä½¿ç”¨ç´¢å¼• i
            o_proj = self.os_proj[i] # ç›´æŽ¥ä½¿ç”¨ç´¢å¼• i

            current_batch_size, current_seq_len, _ = hidden_states.shape
            input_lens.append(current_seq_len)
            # ç¡®ä¿ batch_size ä¸€è‡´
            if current_batch_size != batch_size:
                 raise ValueError(f"Batch size mismatch in multi_hidden_states: expected {batch_size}, got {current_batch_size} for stream {i}")

            # hidden_shape: (batch_size, num_heads, seq_len, head_dim)
            hidden_shape = (current_batch_size, self.config.num_attention_heads, current_seq_len, self.head_dim)
            hidden_shapes.append(hidden_shape)

            # (batch_size, num_heads, seq_len, head_dim)
            query_state = qkv_proj[0](hidden_states).view(hidden_shape)
            # (batch_size, num_key_value_heads, seq_len, head_dim)
            key_state = qkv_proj[1](hidden_states).view(current_batch_size, self.config.num_key_value_heads, current_seq_len, self.head_dim)
            value_state = qkv_proj[2](hidden_states).view(current_batch_size, self.config.num_key_value_heads, current_seq_len, self.head_dim)

            qkvs.append((query_state, key_state, value_state))

        # --- ç»„åˆ QKV ---
        # è¿‡æ»¤æŽ‰ None å€¼å¹¶ç»„åˆ
        valid_qkvs = [qkv for qkv in qkvs if qkv[0] is not None]
        if not valid_qkvs:
             # å¦‚æžœä¸¤ä¸ªæµéƒ½æ˜¯ Noneï¼Œåˆ™æå‰è¿”å›ž
             return (None, None), None

        transposed = list(zip(*valid_qkvs))
        # æ²¿åºåˆ—é•¿åº¦ç»´åº¦æ‹¼æŽ¥: (batch_size, num_heads, total_seq_len, head_dim) for Q
        # (batch_size, num_kv_heads, total_seq_len, head_dim) for K, V
        query_states = torch.cat([q[0] for q in valid_qkvs], dim=2)
        key_states = torch.cat([q[1] for q in valid_qkvs], dim=2)
        value_states = torch.cat([q[2] for q in valid_qkvs], dim=2)

        # --- RoPE ---
        cos, sin = position_embeddings
        # æ³¨æ„ï¼šRoPE åº”ç”¨åœ¨æ‹¼æŽ¥åŽçš„ QK ä¸Šï¼Œéœ€è¦ç¡®ä¿ position_ids/embeddings å¯¹åº”æ‹¼æŽ¥åŽçš„åºåˆ—
        # å¦‚æžœä¸¤ä¸ªæµæœ‰ä¸åŒçš„ position_ids è¯­ä¹‰ï¼Œè¿™é‡Œéœ€è¦è°ƒæ•´
        # å‡è®¾ position_embeddings å·²ç»å¤„ç†å¥½äº†æ‹¼æŽ¥åŽçš„æƒ…å†µ
        query_states, key_states = apply_rotary_pos_emb(
            query_states, key_states, cos, sin
        )

        # --- KV Cache ---
        if past_key_value is not None:
            cache_kwargs = {
                "sin": sin,
                "cos": cos,
                "cache_position": cache_position,
                "sliding_window": self.sliding_window,
            }
            key_states, value_states = past_key_value.update(
                key_states, value_states, self.layer_idx, cache_kwargs
            )
            # Slicing for FA2 with static cache (remains the same)
            if (
                attention_mask is not None
                and self.config._attn_implementation == "flash_attention_2"
            ):
                # ç¡®å®šç”¨äºŽåˆ‡ç‰‡çš„ key_states åºåˆ—é•¿åº¦
                kv_seq_len = key_states.shape[-2] # ä½¿ç”¨æ›´æ–°åŽçš„ K çš„é•¿åº¦
                # attention_mask çš„æœ€åŽä¸€ä¸ªç»´åº¦åº”è¯¥å¯¹åº” kv_seq_len
                # å¦‚æžœ attention_mask æ˜¯ 2D çš„ï¼Œéœ€è¦ç¡®ä¿å…¶é•¿åº¦ä¸Ž kv_seq_len åŒ¹é…æˆ–å¯å¹¿æ’­
                # å¦‚æžœ attention_mask æ˜¯ 4D çš„ï¼Œå…¶æœ€åŽç»´åº¦åº”ä¸º kv_seq_len
                # æ­¤å¤„å‡è®¾ attention_mask ç»´åº¦ä¸Ž kv_seq_len å…¼å®¹
                key_states, value_states = (
                    key_states[:, :, :kv_seq_len, :],
                    value_states[:, :, :kv_seq_len, :],
                )


        # --- Attention è®¡ç®— ---
        attention_interface: Callable = eager_attention_forward
        # ... (é€‰æ‹© attention_interface çš„é€»è¾‘ä¿æŒä¸å˜) ...

        # attn_outputs: (batch_size, num_heads, total_query_seq_len, head_dim)
        attn_outputs, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask, # ç¡®ä¿ attention_mask åŒ¹é…æ‹¼æŽ¥åŽçš„ QK ç»´åº¦
            dropout=self.attention_dropout if self.training else 0.0,
            scaling=self.scaling,
            sliding_window=self.sliding_window,
            softcap=self.attn_logit_softcapping,
            **kwargs,
        )

        # --- æ‹†åˆ†å’Œè¾“å‡ºæŠ•å½± ---
        # attn_outputs: (batch_size, total_query_seq_len, num_heads * head_dim)
        attn_outputs = attn_outputs.transpose(1, 2).reshape(batch_size, sum(l for l in input_lens if l > 0), -1).contiguous()

        outputs = []
        start_idx = 0
        valid_stream_idx = 0 # è·Ÿè¸ªæœ‰æ•ˆæµçš„ç´¢å¼•
        for i in range(len(multi_hidden_states)): # éåŽ†åŽŸå§‹æµçš„æ•°é‡
            if multi_hidden_states[i] is None:
                outputs.append(None) # ä¿ç•™ None å ä½ç¬¦
                continue

            current_seq_len = input_lens[i]
            end_idx = start_idx + current_seq_len

            # åˆ‡ç‰‡ attn_outputs
            attn_output_slice = attn_outputs[:, start_idx:end_idx, :].contiguous()

            # åº”ç”¨å¯¹åº”çš„ O æŠ•å½±
            o_proj = self.os_proj[i]
            attn_output_projected = o_proj(attn_output_slice)
            outputs.append(attn_output_projected)

            start_idx = end_idx
            valid_stream_idx += 1

        outputs = tuple(outputs) # è½¬æ¢å›žå…ƒç»„

        return outputs, attn_weights # attn_weights å¯¹åº”äºŽæ‹¼æŽ¥åŽçš„æ³¨æ„åŠ›


class Gemma2DecoderLayer(nn.Module):
    def __init__(self, config: Gemma2Config, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.action_hidden_size = config.action_expert_config.hidden_size # æ·»åŠ  action hidden size
        self.config = config
        self.is_sliding = not bool(layer_idx % 2)
        self.self_attn = Gemma2Attention(config=config, layer_idx=layer_idx)

        # é‡å‘½å FFN æ¨¡å—ä»¥ä¾¿æ›´æ¸…æ™°åœ°åŒºåˆ†
        self.main_ffn = Gemma2MLP(config)
        self.action_ffn = Gemma2ActionMLP(config.action_expert_config)
        self.ffn_mlps = (self.main_ffn, self.action_ffn) # ä½¿ç”¨å…ƒç»„å­˜å‚¨

        # è¾“å…¥å±‚å½’ä¸€åŒ–
        self.input_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.action_input_layernorm = Gemma2RMSNorm(config.action_expert_config.hidden_size, eps=config.rms_norm_eps)
        self.input_layernorms = (self.input_layernorm, self.action_input_layernorm)

        # æ³¨æ„åŠ›åŽçš„å±‚å½’ä¸€åŒ– (ç”¨äºŽ FFN è¾“å…¥)
        self.post_attention_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_action_attention_layernorm = Gemma2RMSNorm(config.action_expert_config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorms = (self.post_attention_layernorm, self.post_action_attention_layernorm)

        self.sliding_window = config.sliding_window

    def forward(
        self,
        multi_hidden_states: Tuple[Optional[torch.Tensor], Optional[torch.Tensor]], # æ˜Žç¡®ç±»åž‹
        position_embeddings: Tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None, # position_ids æœªåœ¨ Attention ä¸­ä½¿ç”¨ï¼Œä½†ä¿ç•™ä»¥é˜²æœªæ¥éœ€è¦
        past_key_value: Optional[Cache] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        last_cache_position: int = 0,
        **kwargs,
    ) -> Tuple[Tuple[Optional[torch.Tensor], Optional[torch.Tensor]], Optional[torch.Tensor]]: # è°ƒæ•´è¿”å›žç±»åž‹
        # --- Sliding Window Masking (ä¿æŒä¸å˜) ---
        if self.is_sliding and attention_mask is not None:
            # ... (sliding window mask logic) ...
            pass # ä¿æŒåŽŸæœ‰é€»è¾‘

        # --- Pre-Attention Layernorm ---
        residual = multi_hidden_states # ä¿å­˜åŽŸå§‹è¾“å…¥ç”¨äºŽæ®‹å·®è¿žæŽ¥
        normalized_hidden_states = []
        for i, hidden_states in enumerate(multi_hidden_states):
            if hidden_states is None:
                normalized_hidden_states.append(None)
                continue
            layernorm = self.input_layernorms[i] # ç›´æŽ¥ä½¿ç”¨ç´¢å¼• i
            normalized_hidden_states.append(layernorm(hidden_states))
        normalized_hidden_states = tuple(normalized_hidden_states)

        # --- Self Attention ---
        # attn_outputs: ( (attn_output_main, attn_output_action), attn_weights )
        attn_outputs, self_attn_weights = self.self_attn(
            multi_hidden_states=normalized_hidden_states, # ä½¿ç”¨å½’ä¸€åŒ–åŽçš„çŠ¶æ€
            position_embeddings=position_embeddings,
            attention_mask=attention_mask,
            # position_ids=position_ids, # ä¼ é€’ position_ids å¦‚æžœ Attention éœ€è¦
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
            cache_position=cache_position,
            last_cache_position=last_cache_position, # ä¼ é€’ last_cache_position
            **kwargs,
        )

        # --- Post-Attention Residual & Pre-FFN Layernorm ---
        ffn_inputs = []
        for i, (attn_output, res) in enumerate(zip(attn_outputs, residual)):
            if attn_output is None or res is None:
                ffn_inputs.append(None)
                continue
            # ç¬¬ä¸€ä¸ªæ®‹å·®è¿žæŽ¥ (Attention è¾“å‡º + åŽŸå§‹è¾“å…¥)
            hidden_states_after_attn = res + attn_output
            # FFN è¾“å…¥çš„å±‚å½’ä¸€åŒ–
            layernorm = self.post_attention_layernorms[i] # ç›´æŽ¥ä½¿ç”¨ç´¢å¼• i
            ffn_inputs.append(layernorm(hidden_states_after_attn))
        ffn_inputs = tuple(ffn_inputs)

        # --- FFN & Post-FFN Residual ---
        final_hidden_states = []
        for i, ffn_input in enumerate(ffn_inputs):
            if ffn_input is None:
                final_hidden_states.append(None)
                continue

            ffn_mlp = self.ffn_mlps[i] # ç›´æŽ¥ä½¿ç”¨ç´¢å¼• i
            ffn_output = ffn_mlp(ffn_input)

            # ç¬¬äºŒä¸ªæ®‹å·®è¿žæŽ¥ (FFN è¾“å‡º + Attention åŽçš„çŠ¶æ€)
            # éœ€è¦ Attention åŽçš„çŠ¶æ€ï¼Œå³ç¬¬ä¸€ä¸ªæ®‹å·®è¿žæŽ¥çš„ç»“æžœ
            # ä»Ž ffn_inputs åæŽ¨å›ž Attention åŽçš„çŠ¶æ€ï¼ˆåœ¨ layernorm ä¹‹å‰ï¼‰
            # æˆ–è€…ï¼Œæ›´ç®€å•çš„æ–¹å¼æ˜¯é‡æ–°è®¡ç®—ç¬¬ä¸€ä¸ªæ®‹å·®
            hidden_states_after_attn = residual[i] + attn_outputs[i] if residual[i] is not None and attn_outputs[i] is not None else None
            if hidden_states_after_attn is not None:
                 final_hidden_state = hidden_states_after_attn + ffn_output
                 final_hidden_states.append(final_hidden_state)
            else:
                 final_hidden_states.append(None) # å¦‚æžœ Attention åŽçŠ¶æ€ä¸º Noneï¼Œåˆ™ FFN è¾“å‡ºä¹Ÿä¸º None

        final_hidden_states = tuple(final_hidden_states)

        outputs = (final_hidden_states,)
        if output_attentions:
            outputs += (self_attn_weights,)

        # è¿”å›žåŒ…å«ä¸¤ä¸ªæµéšè—çŠ¶æ€çš„å…ƒç»„å’Œæ³¨æ„åŠ›æƒé‡
        return outputs[0], outputs[1] if output_attentions else None

class Gemma2RotaryEmbedding(nn.Module):
    def __init__(self, config: Gemma2Config, device=None):
        super().__init__()
        # BC: "rope_type" was originally "type"
        if hasattr(config, "rope_scaling") and config.rope_scaling is not None:
            self.rope_type = config.rope_scaling.get(
                "rope_type", config.rope_scaling.get("type")
            )
        else:
            self.rope_type = "default"
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config
        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = self.inv_freq

    def _dynamic_frequency_update(self, position_ids, device):
        """
        dynamic RoPE layers should recompute `inv_freq` in the following situations:
        1 - growing beyond the cached sequence length (allow scaling)
        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)
        """
        seq_len = torch.max(position_ids) + 1
        if seq_len > self.max_seq_len_cached:  # growth
            inv_freq, self.attention_scaling = self.rope_init_fn(
                self.config, device, seq_len=seq_len
            )
            self.register_buffer(
                "inv_freq", inv_freq, persistent=False
            )  # TODO joao: may break with compilation
            self.max_seq_len_cached = seq_len

        if (
            seq_len < self.original_max_seq_len
            and self.max_seq_len_cached > self.original_max_seq_len
        ):  # reset
            # This .to() is needed if the model has been moved to a device after being initialized (because
            # the buffer is automatically moved, but not the original copy)
            self.original_inv_freq = self.original_inv_freq.to(device)
            self.register_buffer("inv_freq", self.original_inv_freq, persistent=False)
            self.max_seq_len_cached = self.original_max_seq_len

    @torch.no_grad()
    def forward(self, x, position_ids):
        if "dynamic" in self.rope_type:
            self._dynamic_frequency_update(position_ids, device=x.device)

        # Core RoPE block
        inv_freq_expanded = (
            self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)
        )
        position_ids_expanded = position_ids[:, None, :].float()
        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)
        device_type = x.device.type
        device_type = (
            device_type
            if isinstance(device_type, str) and device_type != "mps"
            else "cpu"
        )
        with torch.autocast(device_type=device_type, enabled=False):
            freqs = (
                inv_freq_expanded.float() @ position_ids_expanded.float()
            ).transpose(1, 2)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos()
            sin = emb.sin()

        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention
        cos = cos * self.attention_scaling
        sin = sin * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


GEMMA2_START_DOCSTRING = r"""
    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
    etc.)

    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
    and behavior.

    Parameters:
        config ([`Gemma2Config`]):
            Model configuration class with all the parameters of the model. Initializing with a config file does not
            load the weights associated with the model, only the configuration. Check out the
            [`~PreTrainedModel.from_pretrained`] method to load the model weights.

    Args:
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the main language modeling loss.
        action_labels (`torch.LongTensor` of shape `(batch_size, action_sequence_length)`, *optional*):
            Labels for computing the action prediction loss.
        # ... (other args)
    Returns:
            # This section will be filled by the decorator
"""


class Gemma2PreTrainedModel(PreTrainedModel):
    config_class = Gemma2Config
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["Gemma2DecoderLayer"]
    _skip_keys_device_placement = ["past_key_values"]
    _supports_flash_attn_2 = True
    _supports_sdpa = True
    _supports_flex_attn = True
    _supports_cache_class = True
    _supports_quantized_cache = True
    _supports_static_cache = True
    _supports_attention_backend = True

    def _init_weights(self, module):
        std = self.config.initializer_range
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()


GEMMA2_INPUTS_DOCSTRING = r"""
    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
            it.

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            [What are attention masks?](../glossary#attention-mask)

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
            `past_key_values`).

            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
            information on the default strategy.

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.
        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
            config.n_positions - 1]`.

            [What are position IDs?](../glossary#position-ids)
        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):
            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.

            Two formats are allowed:
            - a [`~cache_utils.Cache`] instance, see our
            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);
            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of
            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy
            cache format.

            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the
            legacy cache format will be returned.

            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
            of shape `(batch_size, sequence_length)`.
        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
            model's internal embedding lookup matrix.
        use_cache (`bool`, *optional*):
            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
            `past_key_values`).
        output_attentions (`bool`, *optional*):
            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
            tensors for more detail.
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
            more detail.
        return_dict (`bool`, *optional*):
            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
            the complete sequence length.
"""



@add_start_docstrings(
    "The bare Gemma2 Model outputting raw hidden-states without any specific head on top.",
    GEMMA2_START_DOCSTRING,
)
class Gemma2Model(Gemma2PreTrainedModel):
    # ... (init remains largely the same, ensure norm_block is correctly initialized) ...
    def __init__(self, config: Gemma2Config):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(
            config.vocab_size, config.hidden_size, self.padding_idx
        )
        # Action embedder needs its own config size
        self.action_embed_tokens = nn.Embedding(
            config.vocab_size, config.action_expert_config.hidden_size, self.padding_idx
        )
        self.embedders = (self.embed_tokens, self.action_embed_tokens) # Store embedders

        self.layers = nn.ModuleList(
            [
                Gemma2DecoderLayer(config, layer_idx)
                for layer_idx in range(config.num_hidden_layers)
            ]
        )

        self.norm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.action_norm = Gemma2RMSNorm(
            config.action_expert_config.hidden_size, eps=config.rms_norm_eps
        )
        self.final_norms = (self.norm, self.action_norm) # Store final norms
        self.rotary_emb = Gemma2RotaryEmbedding(config=config)
        self.gradient_checkpointing = False

        # Initialize weights and apply final processing
        self.post_init()
    # ... (get/set input embeddings might need adjustment if handling tuples) ...

    @add_start_docstrings_to_model_forward(GEMMA2_INPUTS_DOCSTRING)
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None, # Main input IDs
        action_input_ids: Optional[torch.LongTensor] = None, # Action input IDs (optional)
        inputs_embeds: Optional[Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]] = None, # Allow passing embeds directly
        attention_mask: Optional[torch.Tensor] = None, # Single attention mask for combined sequence
        position_ids: Optional[torch.LongTensor] = None, # Single position_ids for combined sequence
        past_key_values: Optional[HybridCache] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        last_cache_position: Optional[int] = None, # Keep this argument
        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],
    ) -> Union[Tuple, BaseModelOutputWithPast]:
        output_attentions = (
            output_attentions
            if output_attentions is not None
            else self.config.output_attentions
        )
        output_hidden_states = (
            output_hidden_states
            if output_hidden_states is not None
            else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache
        return_dict = (
            return_dict if return_dict is not None else self.config.use_return_dict
        )

        # --- Input Validation and Embedding ---
        if inputs_embeds is None:
            if input_ids is None:
                 raise ValueError("Either 'input_ids' or 'inputs_embeds' for the main stream must be provided.")
            main_embeds = self.embedders[0](input_ids)
            action_embeds = self.embedders[1](action_input_ids) if action_input_ids is not None else None
            inputs_embeds = (main_embeds, action_embeds)
        elif not isinstance(inputs_embeds, tuple) or len(inputs_embeds) != 2:
             raise ValueError("'inputs_embeds' must be a tuple of two elements (main_embeds, action_embeds).")

        # --- Determine Batch Size, Sequence Length, Device ---
        # Find the first non-None embed to determine shape and device
        first_valid_embed = next((embed for embed in inputs_embeds if embed is not None), None)
        if first_valid_embed is None:
            raise ValueError("At least one stream in 'inputs_embeds' must be provided.")
        batch_size, main_seq_len, _ = first_valid_embed.shape # Use main seq len for reference
        device = first_valid_embed.device

        # --- KV Cache Initialization ---
        # Use combined sequence length if needed for cache init
        # Note: HybridCache might need adjustments if sequence lengths differ significantly
        # For now, assume cache is sized based on the main sequence length or a max length
        if use_cache and past_key_values is None and not self.training:
            # Determine max_cache_len based on combined length or config
            # This part might need refinement based on how cache handles different lengths
            combined_seq_len = sum(embed.shape[1] for embed in inputs_embeds if embed is not None)
            max_cache_len = self.config.max_position_embeddings # Or a calculated max length
            past_key_values = HybridCache(
                self.config,
                max_batch_size=batch_size,
                max_cache_len=max_cache_len, # Use appropriate max length
                dtype=first_valid_embed.dtype,
                device=device, # Pass device explicitly
            )

        # --- Position IDs and Cache Position ---
        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        # cache_position should correspond to the *combined* sequence length if inputs are concatenated
        # This requires careful handling if inputs are processed separately before concatenation in Attention
        # Assuming cache_position refers to the state *before* potential concatenation in Attention
        # If inputs are concatenated *before* the first layer, cache_position needs to reflect that.
        # Let's assume cache_position corresponds to the main input stream for now.
        # TODO: Revisit cache_position/position_ids if inputs are concatenated early.
        if cache_position is None:
             # Use the sequence length of the main input for cache_position calculation
             current_seq_len = inputs_embeds[0].shape[1] if inputs_embeds[0] is not None else inputs_embeds[1].shape[1]
             cache_position = torch.arange(
                 past_seen_tokens, past_seen_tokens + current_seq_len, device=device
             )

        if position_ids is None:
            # position_ids should match the sequence length dimension used by RoPE
            # If RoPE is applied *after* concatenation, position_ids need to be combined.
            # If RoPE is applied *before* concatenation (as seems likely), use individual position_ids.
            # Let's assume RoPE uses the position_ids corresponding to the main stream length here.
            # TODO: Clarify RoPE and position_ids handling for multiple streams.
            position_ids = cache_position.unsqueeze(0)


        # --- Attention Mask ---
        # The attention mask needs to be constructed carefully if sequences are concatenated.
        # The provided `_update_causal_mask` likely assumes a single input tensor.
        # If concatenation happens inside Attention, the mask passed here should cover the combined length.
        # If inputs remain separate until Attention, the mask might need internal handling there.
        # Assuming the mask passed corresponds to the *combined* key/value sequence length expected by the attention mechanism.
        # TODO: Verify mask creation aligns with how/when sequences are combined.
        if last_cache_position is None: # Logic for last_cache_position remains the same
             last_cache_position = 0
             # ... (rest of last_cache_position logic)

        causal_mask = self._update_causal_mask(
            attention_mask,
            first_valid_embed, # Use a valid embed for shape/dtype info
            cache_position,
            past_key_values,
            output_attentions,
        )

        # --- Initial Hidden States Preparation ---
        hidden_states_prepared = []
        # RoPE embeddings need careful handling if sequence lengths differ.
        # Assuming RoPE is applied based on the `position_ids` calculated above (likely main stream).
        # If action stream needs different RoPE, this needs adjustment.
        # TODO: Confirm RoPE application logic for multiple streams.
        position_embeddings = self.rotary_emb(first_valid_embed, position_ids) # Calculate RoPE once

        hidden_sizes = [self.config.hidden_size, self.config.action_expert_config.hidden_size]

        for i, input_embed in enumerate(inputs_embeds):
            if input_embed is None:
                hidden_states_prepared.append(None)
                continue

            # Normalize with sqrt(hidden_size)
            hidden_size = hidden_sizes[i]
            normalizer = torch.tensor(hidden_size**0.5, dtype=input_embed.dtype)
            normalized_embed = input_embed * normalizer
            hidden_states_prepared.append(normalized_embed)

        hidden_states = tuple(hidden_states_prepared)

        # --- Decoder Layers ---
        all_hidden_states = () if output_hidden_states else None
        all_self_attns = () if output_attentions else None

        for layer_idx, decoder_layer in enumerate(self.layers): # Use enumerate for index
            if output_hidden_states:
                # Store the *input* hidden state of the layer
                all_hidden_states += (hidden_states,)

            layer_outputs = None # Initialize layer_outputs
            if self.gradient_checkpointing and self.training:
                # Define a wrapper function for gradient checkpointing
                def create_custom_forward(module):
                    def custom_forward(*inputs):
                        # Unpack inputs for the decoder layer call
                        # Ensure the order matches the decoder_layer.forward signature
                        _hidden_states, _pos_embeds, _attn_mask, _pos_ids, _past_kv, _out_attn, _use_c, _cache_pos, _last_cache_pos = inputs
                        # Pass flash_attn_kwargs if needed by the layer
                        return module(
                             _hidden_states,
                             position_embeddings=_pos_embeds,
                             attention_mask=_attn_mask,
                             position_ids=_pos_ids,
                             past_key_value=_past_kv,
                             output_attentions=_out_attn,
                             use_cache=_use_c,
                             cache_position=_cache_pos,
                             last_cache_position=_last_cache_pos,
                             **flash_attn_kwargs # Pass kwargs here
                        )
                    return custom_forward

                # Prepare inputs for checkpointing
                checkpoint_inputs = (
                    hidden_states,
                    position_embeddings,
                    causal_mask,
                    position_ids,
                    past_key_values,
                    output_attentions,
                    use_cache,
                    cache_position,
                    last_cache_position,
                 )
                # Ensure requires_grad is set appropriately for inputs if needed

                # Call gradient checkpointing
                layer_outputs_tuple = torch.utils.checkpoint.checkpoint(
                    create_custom_forward(decoder_layer),
                    *checkpoint_inputs,
                    use_reentrant=not self.config.gradient_checkpointing_kwargs.get("use_reentrant", True) # Use config setting
                 )
                 # Reconstruct layer_outputs structure if necessary
                 # Assuming checkpoint returns ( (hidden_state_tuple), optional_attn_weights )
                layer_outputs = (layer_outputs_tuple[0], layer_outputs_tuple[1] if len(layer_outputs_tuple) > 1 else None)

            else:
                # Direct call
                hidden_states_out, attn_weights_out = decoder_layer(
                    hidden_states,
                    position_embeddings=position_embeddings,
                    attention_mask=causal_mask,
                    position_ids=position_ids,
                    past_key_value=past_key_values,
                    output_attentions=output_attentions,
                    use_cache=use_cache,
                    cache_position=cache_position,
                    last_cache_position=last_cache_position,
                    **flash_attn_kwargs,
                )
                layer_outputs = (hidden_states_out, attn_weights_out)


            hidden_states = layer_outputs[0] # Update hidden_states for the next layer

            if output_attentions and layer_outputs[1] is not None:
                all_self_attns += (layer_outputs[1],)

            # Update past_key_values if use_cache is True
            # The cache object is updated internally by the attention layer if passed correctly
            # No explicit update needed here if using Cache object


        # --- Final Layer Norm ---
        final_hidden_states = []
        for i, hidden_state in enumerate(hidden_states):
             if hidden_state is None:
                 final_hidden_states.append(None)
                 continue
             norm = self.final_norms[i] # Use index i
             final_hidden_states.append(norm(hidden_state))
        hidden_states = tuple(final_hidden_states)


        if output_hidden_states:
            all_hidden_states += (hidden_states,) # Add final normalized states

        # --- Return ---
        if not return_dict:
            return tuple(
                v
                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns]
                if v is not None
            )

        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states, # Return tuple of hidden states
            past_key_values=past_key_values,
            hidden_states=all_hidden_states,
            attentions=all_self_attns,
        )
    
    def _update_causal_mask(
        self,
        attention_mask: torch.Tensor,
        input_tensor: torch.Tensor,
        cache_position: torch.Tensor,
        past_key_values: Cache,
        output_attentions: bool,
    ):
        if self.config._attn_implementation == "flash_attention_2":
            if attention_mask is not None and 0.0 in attention_mask:
                return attention_mask
            if not output_attentions:
                # Targeting is_causal=True is a safe default for Flash Attention version 2.
                # We are processing the input in chunks, potentially leaving some past_kv_length
                # depending on the size of the cache. We need to know the total sequence length
                # to pass it down to the attention function.
                past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
                target_length = cache_position[-1] + 1
                return past_seen_tokens, target_length
            # TODO: Remove this once we can support passing sliding window attention mask to FA2
            # With output_attentions=True, we need a causal mask for eager attention.
            # We can't rely on is_causal=True anymore, so we build the mask here.
            else:
                # TODO: Remove this once we can support passing sliding window attention mask to FA2
                # With output_attentions=True, we need a causal mask for eager attention.
                # We can't rely on is_causal=True anymore, so we build the mask here.
                dtype = input_tensor.dtype
                device = input_tensor.device
                min_dtype = torch.finfo(dtype).min
                sequence_length = input_tensor.shape[1]
                target_sequence_length = (
                    past_key_values.get_max_length() # use cache length
                    if past_key_values is not None
                    else sequence_length
                )

                if attention_mask is not None and attention_mask.dim() == 4:
                    # In this case we assume that the mask comes already in 4D causal format and just return it
                    # Note that in this case the mask is assumed to be correct, i.e. we trust the user that the mask reflects the states in the cache.
                    # TODO: We might want to check shape and dtype here.
                    return attention_mask

                # if past_key_values is not None:
                #     # Assume we have caching enabled and need the full mask
                #     # We cannot assume that the cache position is contiguous, so we use the maximum cache position to determine the total sequence length.
                #     # This might overestimate the sequence length, but is safe.
                #     target_sequence_length = past_key_values.get_max_length()

                causal_mask = torch.full(
                    (sequence_length, target_sequence_length), fill_value=min_dtype, dtype=dtype, device=device
                )
                if sequence_length != 1:
                    causal_mask = torch.triu(causal_mask, diagonal=1)
                causal_mask *= torch.arange(target_sequence_length, device=device) > cache_position.reshape(-1, 1)
                causal_mask = causal_mask[None, None, :, :].expand(input_tensor.shape[0], 1, -1, -1)
                if attention_mask is not None:
                    # We need to take care of padding tokens also. This is somewhat tricky.
                    # We want the final mask to be like this:
                    # [[1, 1, 1, 0, 0],   -> Attend to tokens 0, 1, 2, but not 3, 4
                    #  [1, 1, 1, 0, 0],   -> Attend to tokens 0, 1, 2, but not 3, 4
                    #  [1, 1, 1, 0, 0],   -> Attend to tokens 0, 1, 2, but not 3, 4
                    #  [0, 0, 0, 1, 0],   -> Attend to token 3, but not 0, 1, 2, 4
                    #  [0, 0, 0, 0, 1]]   -> Attend to token 4, but not 0, 1, 2, 3
                    # Let's see what the current causal mask is:
                    # [[0, M, M, M, M],   -> Attend to token 0
                    #  [0, 0, M, M, M],   -> Attend to tokens 0, 1
                    #  [0, 0, 0, M, M],   -> Attend to tokens 0, 1, 2
                    #  [0, 0, 0, 0, M],   -> Attend to tokens 0, 1, 2, 3
                    #  [0, 0, 0, 0, 0]]   -> Attend to tokens 0, 1, 2, 3, 4
                    # We want to get the final mask by combining the causal mask and the attention mask.
                    # We can do that by adding the attention mask to the causal mask.
                    # But we need to be careful with the dimensions.
                    # The attention mask is (batch_size, target_sequence_length)
                    # The causal mask is (batch_size, 1, sequence_length, target_sequence_length)
                    # We need to expand the attention mask to match the causal mask dimensions.
                    expanded_attn_mask = attention_mask[:, None, None, :].expand(
                        input_tensor.shape[0], 1, sequence_length, -1
                    )
                    # Add the expanded attention mask to the causal mask
                    causal_mask = torch.where(expanded_attn_mask == 0, min_dtype, causal_mask)

                # Handle sliding window case
                sliding_window = getattr(self.config, "sliding_window", None)
                if sliding_window is not None:
                    diagonal = past_key_values.get_seq_length() - sequence_length + 1 if past_key_values is not None else 1
                    context_length = torch.arange(target_sequence_length, device=device)[None, :]
                    query_length = torch.arange(sequence_length, device=device)[:, None]
                    sliding_mask = context_length + sliding_window < query_length + diagonal
                    causal_mask = torch.where(sliding_mask[None, None, :, :], min_dtype, causal_mask)

                return causal_mask

        elif self.config._attn_implementation == "sdpa":
            # For SDPA, we need to deal with the different mask formats and caching model
            # We assume the user is using one of the supported cache types
            if past_key_values is None:
                # If we don't have cache, we can leverage SDPA's is_causal=True support
                # TODO: this requires torch>=2.1
                # We need to check the sliding window condition though
                sliding_window = getattr(self.config, "sliding_window", None)
                if sliding_window is None:
                    # If we have an attention mask, we can simply pass it to SDPA
                    # Note that the format should be Boolean, with True indicating which positions to attend to
                    if attention_mask is not None:
                        # SDPA requires a Bool mask, where True indicates attention allowed
                        # Convert 0/1 mask to Bool
                        if attention_mask.dtype != torch.bool:
                             attention_mask = attention_mask.bool()
                        # Ensure 2D mask for SDPA when no cache
                        if attention_mask.dim() == 4:
                             # Select the part relevant for the current query tokens
                             attention_mask = attention_mask[:, 0, -input_tensor.shape[1]:, :]

                        # Check if it's a causal mask (can be optimized)
                        # Simple check: is the lower triangle all True?
                        # This might not cover all causal mask variations but is a start
                        is_causal_mask = (attention_mask.tril() == attention_mask).all() and (attention_mask.tril().diagonal() == 1).all()

                        if is_causal_mask and attention_mask.shape[-1] == input_tensor.shape[1]:
                             # If it's a causal mask matching the input length, let SDPA handle it
                             return None # SDPA will use is_causal=True
                        else:
                             # Otherwise, pass the mask directly
                             return attention_mask
                    else:
                        # No attention mask, implies full causal attention
                        return None # SDPA will use is_causal=True
                else:
                    # With sliding window, we need to build the mask explicitly for SDPA
                    # as `is_causal=True` doesn't support sliding window.
                    return self._prepare_4d_causal_attention_mask(
                        attention_mask,
                        input_tensor.shape,
                        input_tensor,
                        0, # No past length when no cache
                        sliding_window=sliding_window,
                    )
            else:
                # With cache, we need the 4D mask for SDPA
                # We can leverage the existing helper function
                past_seen_tokens = past_key_values.get_seq_length()
                return self._prepare_4d_causal_attention_mask_for_sdpa(
                    attention_mask,
                    (input_tensor.shape[0], input_tensor.shape[1]),
                    input_tensor,
                    past_seen_tokens,
                    sliding_window=getattr(self.config, "sliding_window", None),
                )

        # Default case: Eager implementation or unknown
        else:
            # We need the 4D mask for the eager implementation
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            return self._prepare_4d_causal_attention_mask(
                attention_mask,
                input_tensor.shape,
                input_tensor,
                past_seen_tokens,
                sliding_window=getattr(self.config, "sliding_window", None),
            )

    # Adapted from transformers LlamaModel
    def _prepare_4d_causal_attention_mask(
        self,
        attention_mask: Optional[torch.Tensor],
        input_shape: Union[torch.Size, Tuple, List],
        inputs_embeds: torch.Tensor,
        past_key_values_length: int,
        sliding_window: Optional[int] = None,
    ):
        """
        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D or 3D mask.

        Args:
            attention_mask (`torch.Tensor` or `None`):
                A 2D attention mask of shape `(batch_size, key_value_length)` or a 3D mask of shape
                `(batch_size, query_length, key_value_length)`. Boolean or float type.
            input_shape (`torch.Size` or `tuple` or `list`):
                The shape of the input tensor. `(batch_size, query_length, ...)`
            inputs_embeds (`torch.Tensor`):
                The input embeddings tensor, used to determine the dtype and device.
            past_key_values_length (`int`):
                The length of the key value cache.
            sliding_window (`int`, *optional*):
                If given, applies a sliding window attention mask.
        """
        batch_size, query_length = input_shape[:2]
        dtype = inputs_embeds.dtype
        device = inputs_embeds.device
        min_dtype = torch.finfo(dtype).min

        # 4d mask is passed through the layers
        if attention_mask is not None and attention_mask.dim() == 4:
            # ... (existing logic for 4D mask remains the same) ...
            # Ensure correct dtype
            if attention_mask.dtype != dtype:
                 # Convert boolean 4D mask to float additive mask
                 if attention_mask.dtype == torch.bool:
                     attn_mask_float = torch.zeros_like(attention_mask, dtype=dtype)
                     attn_mask_float.masked_fill_(~attention_mask, min_dtype)
                     attention_mask = attn_mask_float
                 else:
                     attention_mask = attention_mask.to(dtype)
            return attention_mask

        key_value_length = past_key_values_length + query_length

        # Handle 3D mask input (e.g., from make_attn_mask)
        if attention_mask is not None and attention_mask.dim() == 3:
            # Expected shape: (batch_size, query_length, key_value_length)
            expected_shape = (batch_size, query_length, key_value_length)
            if attention_mask.shape != expected_shape:
                # Try slicing if key_value_length dimension is larger
                if attention_mask.shape[0] == expected_shape[0] and \
                   attention_mask.shape[1] == expected_shape[1] and \
                   attention_mask.shape[2] >= expected_shape[2]:
                    attention_mask = attention_mask[:, :, -expected_shape[2]:]
                else:
                    raise ValueError(
                        f"The provided 3D attention mask of shape {attention_mask.shape} does not match the "
                        f"expected shape {expected_shape} derived from inputs and past_key_values."
                    )

            # Convert 3D bool/float mask to 4D float additive mask (B, 1, Q, K)
            if attention_mask.dtype == torch.bool:
                # If bool (True=attend), convert to float (0.0=attend, -inf=mask)
                attn_mask_4d = torch.zeros(batch_size, 1, query_length, key_value_length, dtype=dtype, device=device)
                attn_mask_4d.masked_fill_(~attention_mask[:, None, :, :], min_dtype)
            else:
                # Assume float mask (0.0 or 1.0 = attend, <0 = mask)
                # Add broadcasting dimension and ensure correct dtype
                attn_mask_4d = attention_mask[:, None, :, :].to(dtype)
                # Ensure masked values are min_dtype
                attn_mask_4d = torch.where(attn_mask_4d == 0, torch.zeros_like(attn_mask_4d), attn_mask_4d) # Keep 0s as 0s
                attn_mask_4d = torch.where(attn_mask_4d < 0, min_dtype, attn_mask_4d) # Set negative values to min_dtype

            combined_mask = attn_mask_4d # Use the converted 4D mask

        # Handle 2D mask input
        elif attention_mask is not None and attention_mask.dim() == 2:
            # Expected shape: (batch_size, key_value_length)
            # create causal mask
            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
            # If the user provides an attention mask, we assume it's the full mask, including past keys
            # We need to ensure it has the correct shape
            if attention_mask.shape[0] != batch_size or attention_mask.shape[1] != key_value_length:
                 raise ValueError(
                     f"The provided 2D attention mask of shape {attention_mask.shape} does not match the "
                     f"expected shape ({batch_size}, {key_value_length})."
                 )

            # Convert 2D mask (bool or float) to 4D additive float mask
            if attention_mask.dtype == torch.bool:
                expanded_attn_mask = attention_mask[:, None, None, :].expand(batch_size, 1, query_length, key_value_length)
                inverted_mask = (~expanded_attn_mask) * min_dtype
            else: # Assume float mask (0=masked, 1=attend)
                expanded_attn_mask = attention_mask[:, None, None, :].expand(batch_size, 1, query_length, key_value_length).to(dtype)
                inverted_mask = (1.0 - expanded_attn_mask) * min_dtype


            # Combine with causal mask
            causal_mask = torch.full(
                (query_length, key_value_length), fill_value=min_dtype, dtype=dtype, device=device
            )
            if query_length > 0:
                causal_mask = torch.triu(causal_mask, diagonal=1 + past_key_values_length)

            # Add the inverted mask and the causal mask.
            combined_mask = causal_mask[None, None, :, :] + inverted_mask

        # No attention mask provided
        else:
            # create standard causal mask
            combined_mask = torch.full(
                (batch_size, 1, query_length, key_value_length), fill_value=0.0, dtype=dtype, device=device
            )
            if query_length > 0:
                mask_slice = torch.triu(
                    torch.full((query_length, key_value_length), min_dtype, device=device, dtype=dtype),
                    diagonal=1 + past_key_values_length,
                )
                combined_mask[:, :, :, :] = mask_slice[None, None, :, :]


        if sliding_window is not None:
            # Apply sliding window mask if needed
            local_query_pos = torch.arange(query_length, device=device).view(-1, 1)
            local_key_pos = torch.arange(key_value_length, device=device).view(1, -1)
            relative_pos = local_key_pos - (local_query_pos + past_key_values_length) # Adjust key positions by past length
            # Mask positions outside the sliding window
            sliding_mask_cond = (relative_pos < 0) & (relative_pos < -sliding_window + 1) # Keys before the window start
            # Apply the mask: where sliding_mask_cond is True, set to min_dtype
            combined_mask = torch.where(
                sliding_mask_cond[None, None, :, :],
                min_dtype, # Use min_dtype directly
                combined_mask
            )

        return combined_mask

    # ... rest of the methods (_update_causal_mask, _prepare_4d_causal_attention_mask_for_sdpa, etc.) ...
    # Add this helper if using SDPA and cache
    def _prepare_4d_causal_attention_mask_for_sdpa(
        self,
        attention_mask: Optional[torch.Tensor],
        input_shape: Union[torch.Size, Tuple, List],
        inputs_embeds: torch.Tensor, # Changed from input_dtype to inputs_embeds
        past_key_values_length: int,
        sliding_window: Optional[int] = None,
    ):
        """Prepares the correct 4D attention mask for SDPA."""
        batch_size, query_length = input_shape[:2]
        dtype = inputs_embeds.dtype # Get dtype from inputs_embeds
        device = inputs_embeds.device # Get device from inputs_embeds

        # If the attention mask is already 4D, we don't need to do anything, just ensure it's boolean
        if attention_mask is not None and attention_mask.dim() == 4:
             if attention_mask.dtype == torch.bool:
                 return attention_mask
             else:
                 # Convert float mask (0.0 = attend, -inf = mask) to bool (True = attend, False = mask)
                 return attention_mask > torch.finfo(attention_mask.dtype).min

        key_value_length = past_key_values_length + query_length

        # If no attention mask is provided, create a standard causal mask for SDPA
        if attention_mask is None:
            # `attn_mask` is expected to be of shape (batch_size, num_heads, query_length, key_value_length)
            # For SDPA, the mask should be boolean, True where attention is allowed.
            # Causal mask: lower triangle is True.
            mask = torch.ones(batch_size, 1, query_length, key_value_length, dtype=torch.bool, device=device)
            if query_length > 0:
                 mask = torch.tril(mask, diagonal=past_key_values_length)

            # Apply sliding window if needed
            if sliding_window is not None:
                 local_query_pos = torch.arange(query_length, device=device).view(-1, 1)
                 local_key_pos = torch.arange(key_value_length, device=device).view(1, -1)
                 relative_pos = local_key_pos - (local_query_pos + past_key_values_length)
                 # Mask positions outside the sliding window (True means masked in this condition)
                 sliding_mask_cond = (relative_pos < 0) & (relative_pos < -sliding_window + 1)
                 # Apply the mask: where sliding_mask_cond is True, set mask to False
                 mask = torch.where(sliding_mask_cond[None, None, :, :], False, mask)

            # SDPA doesn't need the num_heads dimension, but Transformers convention often includes it.
            # Let's keep it for consistency for now, although SDPA might ignore it.
            # Or, return shape (bsz, query_length, key_value_length) if that's preferred for SDPA.
            # Let's stick to (bsz, 1, query_length, key_value_length) for now.
            return mask

        # If a 2D attention mask is provided (batch_size, key_value_length)
        elif attention_mask.dim() == 2:
            # Expand the 2D mask to 4D for SDPA: (bsz, 1, query_length, key_value_length)
            # Mask values: True for allowed, False for masked
            if attention_mask.dtype != torch.bool:
                 attention_mask = attention_mask.bool() # Ensure boolean type

            expanded_mask = attention_mask[:, None, None, :].expand(batch_size, 1, query_length, key_value_length)

            # Combine with causal mask
            causal = torch.ones(query_length, key_value_length, dtype=torch.bool, device=device)
            if query_length > 0:
                 causal = torch.tril(causal, diagonal=past_key_values_length)

            # Apply sliding window to causal part if needed
            if sliding_window is not None:
                 local_query_pos = torch.arange(query_length, device=device).view(-1, 1)
                 local_key_pos = torch.arange(key_value_length, device=device).view(1, -1)
                 relative_pos = local_key_pos - (local_query_pos + past_key_values_length)
                 sliding_mask_cond = (relative_pos < 0) & (relative_pos < -sliding_window + 1)
                 causal = torch.where(sliding_mask_cond, False, causal)

            # Final mask is True where both expanded_mask AND causal allow attention
            final_mask = expanded_mask & causal[None, None, :, :]
            return final_mask

        else:
            raise ValueError(f"Attention mask should be 2D or 4D, but is {attention_mask.dim()}D")


    # ... (_update_causal_mask and _prepare_4d_causal_attention_mask_with_cache_position remain the same) ...


class Gemma2ForCausalLM(Gemma2PreTrainedModel, GenerationMixin):
    _tied_weights_keys = ["lm_head.weight"]
    _tp_plan = {"lm_head": "colwise_rep"}
    _pp_plan = {"lm_head": (["hidden_states"], ["logits"])}

    def __init__(self, config):
        super().__init__(config)
        self.model = Gemma2Model(config)
        self.vocab_size = config.vocab_size
        # Main LM head
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        # Action prediction head (ensure output size matches action space)
        # Example: Assuming 14 possible actions
        action_output_dim = 14 # Define action dimension
        self.action_head = nn.Linear(config.action_expert_config.hidden_size, action_output_dim, bias=False)
        self.heads = (self.lm_head, self.action_head) # Store heads

        # Initialize weights and apply final processing
        self.post_init()

    # ... (get/set embeddings, decoder methods remain the same) ...

    @deprecate_kwarg("num_logits_to_keep", version="4.50", new_name="logits_to_keep")
    # @add_start_docstrings_to_model_forward(GEMMA2_INPUTS_DOCSTRING)
    # @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        action_input_ids: Optional[torch.LongTensor] = None, # Add action_input_ids
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[HybridCache] = None,
        inputs_embeds: Optional[Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]] = None, # Expect tuple
        labels: Optional[torch.LongTensor] = None, # Main LM labels
        action_labels: Optional[torch.LongTensor] = None, # Action labels (optional)
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        **loss_kwargs, # Pass loss_kwargs to model potentially
    ) -> Union[Tuple, CausalLMOutputWithPast]:
        r"""
        Args:
            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
                Labels for computing the main language modeling loss.
            action_labels (`torch.LongTensor` of shape `(batch_size, action_sequence_length)`, *optional*):
                Labels for computing the action prediction loss.
            # ... (other args)
        """
        # ... (warning for attn_implementation remains the same) ...
        output_attentions = (
            output_attentions
            if output_attentions is not None
            else self.config.output_attentions
        )
        output_hidden_states = (
            output_hidden_states
            if output_hidden_states is not None
            else self.config.output_hidden_states
        )
        return_dict = (
            return_dict if return_dict is not None else self.config.use_return_dict
        )

        # Pass inputs to the base model
        outputs = self.model(
            input_ids=input_ids,
            action_input_ids=action_input_ids, # Pass action_input_ids
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=True, # Force return_dict for easier access
            cache_position=cache_position,
            # Pass flash_attn_kwargs if they exist in loss_kwargs or are passed directly
            **{k: v for k, v in loss_kwargs.items() if k in inspect.signature(self.model.forward).parameters},
        )

        # outputs.last_hidden_state is now a tuple: (main_hidden_state, action_hidden_state)
        hidden_states_tuple = outputs.last_hidden_state

        # --- Logits Calculation ---
        # Handle logits_to_keep slicing
        slice_indices = slice(None) # Default to keeping all
        if isinstance(logits_to_keep, int) and logits_to_keep > 0:
             slice_indices = slice(-logits_to_keep, None)
        elif isinstance(logits_to_keep, torch.Tensor):
             slice_indices = logits_to_keep # Assume tensor contains indices

        all_logits = []
        for i, hidden_state in enumerate(hidden_states_tuple):
            if hidden_state is None:
                all_logits.append(None) # Keep None placeholder
                continue

            head = self.heads[i] # Select the correct head
            # Apply slicing *before* the head
            sliced_hidden_state = hidden_state[:, slice_indices, :]
            logit = head(sliced_hidden_state)

            # Apply softcapping if configured (only for main LM head usually)
            if i == 0 and self.config.final_logit_softcapping is not None: # Apply only to main logits
                logit = logit / self.config.final_logit_softcapping
                logit = torch.tanh(logit)
                logit = logit * self.config.final_logit_softcapping
            all_logits.append(logit)

        logits = tuple(all_logits) # (main_logits, action_logits)

        # --- Loss Calculation ---
        loss = None
        total_loss = 0.0
        loss_components = {}

        # Main LM Loss
        if labels is not None and logits[0] is not None:
            # Shift so that tokens < n predict n
            shift_logits = logits[0][..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = nn.CrossEntropyLoss()
            shift_logits = shift_logits.view(-1, self.config.vocab_size)
            shift_labels = shift_labels.view(-1)
            # Enable model parallelism
            shift_labels = shift_labels.to(shift_logits.device)
            main_loss = loss_fct(shift_logits, shift_labels)
            total_loss += main_loss
            loss_components["main_lm_loss"] = main_loss.item() # Store loss value

        # Action Prediction Loss (Example)
        if action_labels is not None and logits[1] is not None:
             # Assume action logits/labels correspond to the action sequence length
             # Adjust slicing/shifting based on the specific task
             action_logits = logits[1].contiguous() # Or apply shifting if needed
             action_labels_flat = action_labels.contiguous().view(-1)
             # Flatten the tokens
             loss_fct_action = nn.CrossEntropyLoss() # Or a different loss if needed
             action_logits_flat = action_logits.view(-1, self.action_head.out_features)
             # Enable model parallelism
             action_labels_flat = action_labels_flat.to(action_logits_flat.device)
             action_loss = loss_fct_action(action_logits_flat, action_labels_flat)
             # You might want to weight the action loss
             action_loss_weight = loss_kwargs.get("action_loss_weight", 1.0)
             total_loss += action_loss * action_loss_weight
             loss_components["action_loss"] = action_loss.item() # Store loss value


        if loss_components:
             loss = total_loss


        if not return_dict:
            # Ensure output structure matches expected tuple format
            output = (logits,) + outputs[1:] # outputs[1:] contains past_kv, hidden_states, attentions
            return (loss,) + output if loss is not None else output

        # Include individual losses in the output object if desired
        output_obj = CausalLMOutputWithPast(
            loss=loss,
            logits=logits, # Return tuple of logits
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states, # This will be a tuple of tuples if output_hidden_states=True
            attentions=outputs.attentions, # This will be a tuple of attention tensors
        )
        # Add custom loss components if needed
        # output_obj.loss_components = loss_components
        return output_obj

    # ... (prepare_inputs_for_generation needs significant changes to handle two input streams) ...
    # This method requires careful adaptation based on how generation should handle the two streams.
    # It might involve padding/masking strategies if lengths differ, or separate generation steps.
    # Skipping detailed refactoring of prepare_inputs_for_generation for now as it depends heavily on the generation strategy.

# ... (ActionExpertConfig, make_attn_mask, test functions remain the same) ...

# Add import inspect at the top if not already present
import inspect

__all__ = ["Gemma2ForCausalLM", "Gemma2Model", "Gemma2PreTrainedModel"]


@dataclass
class ActionExpertConfig:

    hidden_size = 1024
    intermediate_size = 2048
    hidden_activation = "gelu_pytorch_tanh"


def make_attn_mask(input_mask, mask_ar):
    """PyTorch version of attention mask creator.

    Tokens can attend to valid inputs tokens which have a cumulative mask_ar
    smaller or equal to theirs. This way `mask_ar` bool[?B, N] can be used to
    setup several types of attention, for example:

      [[1 1 1 1 1 1]]: pure causal attention.

      [[0 0 0 1 1 1]]: prefix-lm attention. The first 3 tokens can attend between
          themselves and the last 3 tokens have a causal attention. The first
          entry could also be a 1 without changing behaviour.

      [[1 0 1 0 1 0 0 1 0 0]]: causal attention between 4 blocks. Tokens of a
          block can attend all previous blocks and all tokens on the same block.

    Args:
      input_mask: bool[B, N] true if its part of the input, false if padding.
      mask_ar: bool[?B, N] mask that's true where previous tokens cannot depend on
        it and false where it shares the same attention mask as the previous token.
    """
    mask_ar = torch.broadcast_to(mask_ar, input_mask.shape)
    cumsum = torch.cumsum(mask_ar, dim=1)
    attn_mask = cumsum[:, None, :] <= cumsum[:, :, None]
    valid_mask = input_mask[:, None, :] * input_mask[:, :, None]
    return torch.logical_and(attn_mask, valid_mask)


def test_gemma2_model():
    """
    æµ‹è¯•Gemma2Modelçš„åŸºæœ¬åŠŸèƒ½
    """
    print("å¼€å§‹æµ‹è¯• Gemma2Model...")

    # 1. åˆ›å»ºä¸€ä¸ªå°åž‹é…ç½®
    action_expert_config = ActionExpertConfig()

    config = Gemma2Config(
        action_expert_config=action_expert_config,
    )

    # 2. åˆå§‹åŒ–æ¨¡åž‹
    print("åˆå§‹åŒ–æ¨¡åž‹...")
    model = Gemma2Model(config)

    # 3. å‡†å¤‡æµ‹è¯•è¾“å…¥
    batch_size = 2
    seq_len = 10
    device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")

    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    model = model.to(device)

    # éšæœºç”Ÿæˆè¾“å…¥ID
    input_ids = torch.randint(
        0, config.vocab_size, (batch_size, seq_len), device=device
    )

    # åˆ›å»ºactionè¾“å…¥
    action_input_ids = torch.randint(
        0, config.vocab_size, (batch_size, seq_len // 2), device=device
    )

    # 4. è¿è¡Œæ¨¡åž‹
    print("è¿è¡Œæ¨¡åž‹å‰å‘ä¼ æ’­...")
    with torch.no_grad():
        # ä½¿ç”¨æ¨¡åž‹çš„embed_tokensç”Ÿæˆinputs_embeds
        main_embeds = model.embed_tokens(input_ids)
        action_embeds = model.action_embed_tokens(action_input_ids)
        # time_embeds =
        # åˆ›å»ºinput_embedså…ƒç»„
        inputs_embeds = (main_embeds, action_embeds)

        prefix_mask = torch.ones((batch_size, seq_len), device=device)
        prefix_ar_mask = torch.zeros((batch_size, seq_len), device=device)

        subfix_mask = torch.ones((batch_size, seq_len // 2), device=device)
        subfix_ar_mask = torch.cat(
            (
                torch.ones((batch_size, 2), device=device),
                torch.zeros((batch_size, seq_len // 2 - 2), device=device),
            ),
            dim=1,
        )

        input_mask = torch.cat((prefix_mask, subfix_mask), dim=1)
        mask_ar = torch.cat((prefix_ar_mask, subfix_ar_mask), dim=1)
        attention_mask = make_attn_mask(input_mask, mask_ar).to(device)
        # åˆ›å»ºæ³¨æ„åŠ›æŽ©ç 
        # attention_mask = torch.ones_like(input_ids[:, :], device=device)
        position_ids = (
            torch.sum(prefix_mask, axis=-1)[:, None]
            + torch.cumsum(subfix_mask, axis=-1)
            - 1
        )

        # å‰å‘ä¼ æ’­
        outputs = model(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            # position_ids = position_ids,
            use_cache=True,
            return_dict=True,
        )

    # 5. éªŒè¯è¾“å‡º
    print("\nè¾“å‡ºéªŒè¯:")
    print(f"last_hidden_stateç±»åž‹: {type(outputs.last_hidden_state)}")
    if isinstance(outputs.last_hidden_state, tuple):
        for i, hidden in enumerate(outputs.last_hidden_state):
            print(f"  hidden_state[{i}] å½¢çŠ¶: {hidden.shape}")
    else:
        print(f"  last_hidden_state å½¢çŠ¶: {outputs.last_hidden_state.shape}")

    print(f"past_key_valuesç±»åž‹: {type(outputs.past_key_values)}")
    print(f"æ¨¡åž‹æ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters())}")

    print("æµ‹è¯•å®Œæˆï¼")


def test_gemma2_for_causal_lm():
    print("å¼€å§‹æµ‹è¯• Gemma2ForCausalLM...")

    # 1. åˆ›å»ºä¸€ä¸ªå°åž‹é…ç½®
    action_expert_config = ActionExpertConfig()

    config = Gemma2Config(
        action_expert_config=action_expert_config,
    )

    # 2. åˆå§‹åŒ–æ¨¡åž‹
    print("åˆå§‹åŒ–æ¨¡åž‹...")
    model = Gemma2ForCausalLM(config)

    # 3. å‡†å¤‡æµ‹è¯•è¾“å…¥
    batch_size = 2
    seq_len = 10
    device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")

    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    model = model.to(device)

    # éšæœºç”Ÿæˆè¾“å…¥ID
    input_ids = torch.randint(
        0, config.vocab_size, (batch_size, seq_len), device=device
    )

    # åˆ›å»ºactionè¾“å…¥
    action_input_ids = torch.randint(
        0, config.vocab_size, (batch_size, seq_len // 2), device=device
    )

    # 4. è¿è¡Œæ¨¡åž‹
    print("è¿è¡Œæ¨¡åž‹å‰å‘ä¼ æ’­...")
    with torch.no_grad():
        # ä½¿ç”¨æ¨¡åž‹çš„embed_tokensç”Ÿæˆinputs_embeds
        main_embeds = model.get_input_embeddings()(input_ids)
        action_embeds = model.model.action_embed_tokens(action_input_ids)

        # åˆ›å»ºinput_embedså…ƒç»„
        inputs_embeds = (main_embeds, action_embeds)

        prefix_mask = torch.ones((batch_size, seq_len), device=device)
        prefix_ar_mask = torch.zeros((batch_size, seq_len), device=device)

        subfix_mask = torch.ones((batch_size, seq_len // 2), device=device)
        subfix_ar_mask = torch.cat(
            (
                torch.ones((batch_size, 2), device=device),
                torch.zeros((batch_size, seq_len // 2 - 2), device=device),
            ),
            dim=1,
        )

        input_mask = torch.cat((prefix_mask, subfix_mask), dim=1)
        mask_ar = torch.cat((prefix_ar_mask, subfix_ar_mask), dim=1)
        attention_mask = make_attn_mask(input_mask, mask_ar).to(device)

        position_ids = (
            torch.sum(prefix_mask, axis=-1)[:, None]
            + torch.cumsum(subfix_mask, axis=-1)
            - 1
        )

        # åˆ›å»ºæ ‡ç­¾ç”¨äºŽæµ‹è¯•æŸå¤±è®¡ç®—
        # labels = torch.randint(0, config.vocab_size, (batch_size, seq_len), device=device)

        # å‰å‘ä¼ æ’­
        outputs = model(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            # position_ids=position_ids,
            use_cache=True,
            return_dict=True,
            # labels=labels
        )

    # 5. éªŒè¯è¾“å‡º
    print("\nè¾“å‡ºéªŒè¯:")
    print(f"logitsç±»åž‹: {type(outputs.logits)}")
    print(f"logitså½¢çŠ¶: {outputs.logits[0].shape, outputs.logits[1].shape}")
    # print(f"losså€¼: {outputs.loss.item()}")

    # print(f"past_key_valuesç±»åž‹: {type(outputs.past_key_values)}")

    if outputs.hidden_states is not None:
        print(f"hidden_statesåŒ…å« {len(outputs.hidden_states[0])} å±‚çš„éšè—çŠ¶æ€")

    print(f"æ¨¡åž‹æ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters())}")

    print("æµ‹è¯•å®Œæˆï¼")


# if __name__ == "__main__":
#     # test_gemma2_model()
#     test_gemma2_for_causal_lm()
