# VLA-X
<p align="center">
    <img src="https://raw.githubusercontent.com/WilliamBUG/VLA-X/main/assets/logo.JPG" width="300"/>
<p>

# Welcome to VLA-X! ğŸš€  

Hello, embodiment intelligence and AI researchers! Welcome to **VLA-X**, an open-source project dedicated to exploring and advancing **Vision-Language-Action (VLA) models**.  

With the rapid evolution of **large language models (LLMs)** and **multimodal AI**, we believe their **generalization potential** can significantly impact complex robotic tasks. Recent advancements in **Vision-Language-Action Models** have shown **remarkable performance in generalizing across diverse robotic control tasks**, leveraging large-scale **pretraining** and **task-specific post-training** strategies. Open-source projects like [**OpenVLA**](https://github.com/openvla/openvla) and [**Openpi**](https://github.com/Physical-Intelligence/openpi) have made tremendous contributions by sharing **code, datasets, and models**, fostering a collaborative environment for further research.  

**VLA-X** is here to push this forward! This project is built with the following key goals:  

* ğŸ”¬ **Validation**: We aim to **reproduce, evaluate, and compare** different open-source datasets, model architectures, and training frameworks, sharing insights with the community.  

* ğŸ— **Construction**: As open-source Vision-Language Models (VLMs) evolve rapidly, it is crucial for VLA models to integrate the best VLM foundations. **VLA-X** provides a modular and user-friendly framework that enables seamless **VLM replacement** for both **pretraining and fine-tuning**.  

* ğŸ“– **Research**: We actively share our **ideas, findings, and latest research progress** in VLA, fostering an open dialogue with the community.  

* ğŸ“ **Tutorials & Learning**: Once our framework reaches maturity, we will release a series of **tutorials and guides**, making it easier for the community to **understand, experiment, and contribute** to VLA research.  

Letâ€™s build the future of **embodied intelligence**â€”one open-source step at a time. ğŸš€ğŸ”¥  

<p align="center">
    <img src="https://raw.githubusercontent.com/WilliamBUG/VLA-X/main/assets/intro.jpg" width="60%"/>
<p>


# ğŸ“ **VLA-X Roadmap** ğŸš€  

Welcome to the **VLA-X Roadmap**! Here, we outline our **milestones and upcoming developments** to keep the community updated on where we are headed. We aim to **validate**, **construct**, **research**, and **educate** on VLA models, ensuring the framework is **modular, scalable, and research-friendly**.  

---

### ğŸ“Œ **Phase 1: Foundation & Validation (Q1 2025)**
âœ… Initiate the **VLA-X project**, documentation, and initial codebase  
âœ… Initiate the first VLA model training with **SOTA VLMs** (Training started at 2025.02.22 with Qwen2.5-VL under OpenVLA framework).  
â¬œ Publish **initial reproduction results** with different models and datasets  
â¬œ Open discussion on **key findings in reproduction**  

---

### âš™ï¸ **Phase 2: Framework Construction (Q2 2025)**
â¬œ Build a **modular VLA framework** that supports **multiple SOTA VLM backbones**  
â¬œ Design **plug-and-play** interfaces for swapping **different VLMs**  
â¬œ Implement **pretraining & fine-tuning pipelines** for **easy customization**  
â¬œ Release **first framework version (v0.1) with baseline models**  


# ğŸŒŸ **Contributors**  

VLA-X is currently supported by two researchers from both **industry and academia**. We aim to foster cross-disciplinary discussions and collaboration between the industrial and research communities, ensuring that VLA-X remains both practical and effective for real-world applications.

### ğŸ† **Core Team**  

- ![@YourGitHub](https://github.com/WilliamBUG.png?size=100) **Tan Xiaoyu** ([@YourGitHub](https://github.com/WilliamBUG)) â€“ **Project Lead**  
  *AI researcher focusing on LLMs and VLMs and their industrial applications. He is currently an algorithm scientist in INF Technology.*  

- ![@User1](https://github.com/User1.png?size=100) **Wang Jinghe** ([@User1](https://github.com/User1)) â€“ **Model Engineering, Training Pipeline**  
  *Ph.D. in Robotics with expertise in LLMs, agent, multimodal learning, and deep learning architectures for robotics.*  


### **ğŸŒŸ Get Involved!**
We **welcome contributions** from researchers, developers, and AI enthusiasts. If youâ€™re passionate about **VLA research**, feel free to:  
â­ **Star this repo â­** to show your support!  
ğŸš€ **Try out the framework** and share feedback!  
ğŸ›  **Fork and contribute** â€“ Every contribution counts!  
ğŸ’¡ **Discuss new ideas & research directions** 

Stay tuned for updates, and letâ€™s build the future of **Vision-Language-Action intelligence** together! ğŸš€ğŸ”¥  





