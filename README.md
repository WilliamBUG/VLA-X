# VLA-X
<p align="center">
    <img src="https://raw.githubusercontent.com/WilliamBUG/VLA-X/main/assets/logo.JPG" width="300"/>
<p>

# Welcome to VLA-X! 🚀  

Hello, embodiment intelligence and AI researchers! Welcome to **VLA-X**, an open-source project dedicated to exploring and advancing **Vision-Language-Action (VLA) models**.  

With the rapid evolution of **large language models (LLMs)** and **multimodal AI**, we believe their **generalization potential** can significantly impact complex robotic tasks. Recent advancements in **Vision-Language-Action Models** have shown **remarkable performance in generalizing across diverse robotic control tasks**, leveraging large-scale **pretraining** and **task-specific post-training** strategies. Open-source projects like [**OpenVLA**](https://github.com/openvla/openvla) and [**Openpi**](https://github.com/Physical-Intelligence/openpi) have made tremendous contributions by sharing **code, datasets, and models**, fostering a collaborative environment for further research.  

**VLA-X** is here to push this forward! This project is built with the following key goals:  

* 🔬 **Validation**: We aim to **reproduce, evaluate, and compare** different open-source datasets, model architectures, and training frameworks, sharing insights with the community.  

* 🏗 **Construction**: As open-source Vision-Language Models (VLMs) evolve rapidly, it is crucial for VLA models to integrate the best VLM foundations. **VLA-X** provides a modular and user-friendly framework that enables seamless **VLM replacement** for both **pretraining and fine-tuning**.  

* 📖 **Research**: We actively share our **ideas, findings, and latest research progress** in VLA, fostering an open dialogue with the community.  

* 🎓 **Tutorials & Learning**: Once our framework reaches maturity, we will release a series of **tutorials and guides**, making it easier for the community to **understand, experiment, and contribute** to VLA research.  

Let’s build the future of **embodied intelligence**—one open-source step at a time. 🚀🔥  

<p align="center">
    <img src="https://raw.githubusercontent.com/WilliamBUG/VLA-X/main/assets/intro.jpg" width="60%"/>
<p>


# 📍 **VLA-X Roadmap** 🚀  

Welcome to the **VLA-X Roadmap**! Here, we outline our **milestones and upcoming developments** to keep the community updated on where we are headed. We aim to **validate**, **construct**, **research**, and **educate** on VLA models, ensuring the framework is **modular, scalable, and research-friendly**.  

---

### 📌 **Phase 1: Foundation & Validation (Q1 2025)**
✅ Initiate the **VLA-X project**, documentation, and initial codebase  
✅ Initiate the first VLA model training with **SOTA VLMs** (Training started at 2025.02.22 with Qwen2.5-VL under OpenVLA framework).  
⬜ Publish **initial reproduction results** with different models and datasets  
⬜ Open discussion on **key findings in reproduction**  

---

### ⚙️ **Phase 2: Framework Construction (Q2 2025)**
⬜ Build a **modular VLA framework** that supports **multiple SOTA VLM backbones**  
⬜ Design **plug-and-play** interfaces for swapping **different VLMs**  
⬜ Implement **pretraining & fine-tuning pipelines** for **easy customization**  
⬜ Release **first framework version (v0.1) with baseline models**  


# 🌟 **Contributors**  

VLA-X is currently supported by two researchers from both **industry and academia**. We aim to foster cross-disciplinary discussions and collaboration between the industrial and research communities, ensuring that VLA-X remains both practical and effective for real-world applications.

### 🏆 **Core Team**  

- ![@YourGitHub](https://github.com/WilliamBUG.png?size=100) **Tan Xiaoyu** ([@YourGitHub](https://github.com/WilliamBUG)) – **Project Lead**  
  *AI researcher focusing on LLMs and VLMs and their industrial applications. He is currently an algorithm scientist in INF Technology.*  

- ![@User1](https://github.com/User1.png?size=100) **Wang Jinghe** ([@User1](https://github.com/User1)) – **Model Engineering, Training Pipeline**  
  *Ph.D. in Robotics with expertise in LLMs, agent, multimodal learning, and deep learning architectures for robotics.*  


### **🌟 Get Involved!**
We **welcome contributions** from researchers, developers, and AI enthusiasts. If you’re passionate about **VLA research**, feel free to:  
⭐ **Star this repo ⭐** to show your support!  
🚀 **Try out the framework** and share feedback!  
🛠 **Fork and contribute** – Every contribution counts!  
💡 **Discuss new ideas & research directions** 

Stay tuned for updates, and let’s build the future of **Vision-Language-Action intelligence** together! 🚀🔥  





