# VLA-X
<p align="center">
    <img src="https://raw.githubusercontent.com/WilliamBUG/VLA-X/main/assets/logo.JPG" width="300"/>
<p>

# Welcome to VLA-X! 🚀  

Hello, embodiment intelligence and AI researchers! Welcome to **VLA-X**, an open-source project dedicated to exploring and advancing **Vision-Language-Action (VLA) models**.  

With the rapid evolution of **large language models (LLMs)** and **multimodal AI**, we believe their **generalization potential** can significantly impact complex robotic tasks. Recent advancements in **Vision-Language-Action Models** have shown **remarkable performance in generalizing across diverse robotic control tasks**, leveraging large-scale **pretraining** and **task-specific post-training** strategies. Open-source projects like [**OpenVLA**](https://github.com/openvla/openvla) and [**Openpi**](https://github.com/Physical-Intelligence/openpi) have made tremendous contributions by sharing **code, datasets, and models**, fostering a collaborative environment for further research.  

**VLA-X** is here to push this forward! This project is built with the following key goals:  

* 🔬 **Validation**: We aim to **reproduce, evaluate, and compare** different open-source datasets, model architectures, and training frameworks, sharing insights with the community.  

* 🏗 **Construction**: As open-source Vision-Language Models (VLMs) evolve rapidly, it is crucial for VLA models to integrate the best VLM foundations. **VLA-X** provides a modular and user-friendly framework that enables seamless **VLM replacement** for both **pretraining and fine-tuning**.  

* 📖 **Research**: We actively share our **ideas, findings, and latest research progress** in VLA, fostering an open dialogue with the community.  

* 🎓 **Tutorials & Learning**: Once our framework reaches maturity, we will release a series of **tutorials and guides**, making it easier for the community to **understand, experiment, and contribute** to VLA research.  
ß
Let’s build the future of **embodied intelligence**—one open-source step at a time. 🚀🔥  

<p align="center">
    <img src="https://raw.githubusercontent.com/WilliamBUG/VLA-X/main/assets/intro.jpg" width="60%"/>
<p>


# 📍 **VLA-X Roadmap** 🚀  

Welcome to the **VLA-X Roadmap**! Here, we outline our **milestones and upcoming developments** to keep the community updated on where we are headed. We aim to **validate**, **construct**, **research**, and **educate** on VLA models, ensuring the framework is **modular, scalable, and research-friendly**.  

---

### 📌 **Phase 1: Foundation & Validation (Before May,2025)**
✅ Kick off the VLA-X project, with initial documentation and codebase setup 🥳   
✅ Implement the OpenPI-0 (Torch version) model architecture based on Paligemma 🥳  
⬜ Reproduce baseline results across different models and datasets for validation   
⬜ Develop an initial modular VLA framework, allowing easy VLM swapping

🎯 **Goal**: Establish a working prototype that supports reproduction and initial training of vision-language models (VLMs), with modularity to enable flexible model replacement.

---

### ⚙️ **Phase 2: Framework Expansion & Complex Task Research (Q2 2025)**

⬜ Expand the VLA framework to support multiple SOTA VLM backbones   
⬜ Design plug-and-play interfaces for easily switching between VLMs   
⬜ Build robust pretraining & fine-tuning pipelines for task-level customization   
⬜ Release VLA-X v0.1, bundled with reproducible baseline models and workflows   
⬜ Start exploration on data reconstruction and collection for complex, real-world tasks   
⬜ Conduct research on model optimization & training efficiency improvements   

🎯 **Goal***: Evolve VLA-X into a flexible, research-ready framework capable of tackling real-world vision-language tasks, while optimizing data workflows and model training efficiency.

# 🌟 **Contributors**  

**VLA-X** is currently supported by two researchers from both **industry and academia**. We aim to foster cross-disciplinary discussions and collaboration between the industrial and research communities, ensuring that VLA-X remains both practical and effective for real-world applications.

🛠 All contributors have **contributed equally** to the development and progress of VLA-X. Contributions are listed in no particular order.

### 🏆 **Core Team**  

- 👨‍💻 **Tan Xiaoyu** ([@GoogleScholar](https://scholar.google.com/citations?user=ftq5rBYAAAAJ&hl=en)) – **Project Lead**  
  *AI researcher specializing in LLMs and VLMs, with a focus on their industrial applications. He is currently an algorithm scientist at INF Technology.*  

- 👨‍💻 **Wang Jinghe** ([@Github](https://github.com/Notfound-JH)) – **Model Engineering, Training Pipeline**  
  *Researcher in robotics with expertise in LLMs, agents, multimodal learning, and deep learning architectures. She holds an M.S. from Tsinghua University.*


# **🌟 Get Involved!**

We **welcome contributions** from researchers, developers, and AI enthusiasts. If you’re passionate about **VLA research**, feel free to:  
⭐ **Star this repo ⭐** to show your support!  
🚀 **Try out the framework** and share feedback!  
🛠 **Fork and contribute** – Every contribution counts!  
💡 **Discuss new ideas & research directions** 

We would like to invite more contributors around **Q2 2025**! Stay tuned for updates, and let’s build the future of **Embodied Intelligence** together! 🚀🔥  





